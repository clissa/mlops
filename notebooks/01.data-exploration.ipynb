{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d423bb-e164-4cd2-9f3e-ca3e265b3925",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This notebook contains initial Exploratory Data Analysis code. This is typically the first step in a data analysis pipeline and it is fundamental to:\n",
    " - get acquainted with the use-case\n",
    " - understand the nature and format of the data\n",
    " - explore the information and noise contained in the data\n",
    " - get insights of challenges we may face when training\n",
    "\n",
    "## Problem\n",
    "\n",
    "This dataset contains a Monte Carlo simulation of $\\rho^{\\pm} \\rightarrow \\pi^{\\pm} + \\pi^0$ decays and the corresponding detector response. Specifically, the data report the measured response of **i) tracker** and **ii) calorimeter**, along with the true pyshical quantitites that generated those measurements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This means that we expect one track per event, with mainly two energy blobs (clusters of cells) in the calorimeter.\n",
    "</div>\n",
    "\n",
    "The final **goal** is to associate the cell signals observed in the calorimeter to the track that caused those energy deposits.\n",
    "\n",
    "## Method\n",
    "\n",
    "The idea is to leverage a **point cloud** data representation to combine tracker and calorimeter information so to associate cell hits to the corresponding track. We will use a [**PointNet**](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) model that is capable of handling this type of data, framed as a **semantic segmentation** approach. More precisely, this means that:\n",
    "- we represent each hit in the detector as a point in the point cloud: x, y, z coordinates + additional features (\"3+\"-dimensional point)\n",
    "- the **learning task** will be binary classification at hit level: for each cell the model learns whether its energy comes mostly from the track (class 1) or not (class 0)\n",
    "\n",
    "## Data structure\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This dataset is organized as follows:\n",
    " - for each event, we create a **sample** (i.e. point cloud)\n",
    " - each sample contains all hits in a cone around a track of the event, called **focal track**\n",
    "     - the cone includes all hits within some $\\Delta R$ distance of the track\n",
    "     - if an event has multiple tracks, then we have more samples per event\n",
    "     - since different samples have possibly different number of hits, **we pad all point clouds to ensure they have same size** (needed since the model requires inputs of same size)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7873b320-dee0-4c09-82d4-c128727a0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_BASEPATH = Path().cwd().parent\n",
    "DATA_PATH = REPO_BASEPATH / \"pnet_data/raw/rho_small.npz\"\n",
    "\n",
    "events = np.load(DATA_PATH)[\"feats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e24c7bb-dc62-45ae-b21e-39e0f0448e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data format and shape:\n",
      "<class 'numpy.ndarray'>\tevents.shape=(325, 800)\n",
      "\n",
      "\n",
      "Column\tdtype\n",
      "event_number\t:<i4\n",
      "cell_ID\t:<i4\n",
      "track_ID\t:<i4\n",
      "delta_R\t:<f4\n",
      "truth_cell_focal_fraction_energy\t:<f4\n",
      "truth_cell_non_focal_fraction_energy\t:<f4\n",
      "truth_cell_neutral_fraction_energy\t:<f4\n",
      "truth_cell_total_energy\t:<f4\n",
      "category\t:|i1\n",
      "track_num\t:<i4\n",
      "x\t:<f4\n",
      "y\t:<f4\n",
      "z\t:<f4\n",
      "distance\t:<f4\n",
      "normalized_x\t:<f4\n",
      "normalized_y\t:<f4\n",
      "normalized_z\t:<f4\n",
      "normalized_distance\t:<f4\n",
      "cell_sigma\t:<f4\n",
      "track_chi2_dof\t:<f4\n",
      "track_chi2_dof_cell_sigma\t:<f4\n",
      "cell_E\t:<f4\n",
      "normalized_cell_E\t:<f4\n",
      "track_pt\t:<f4\n",
      "normalized_track_pt\t:<f4\n",
      "track_pt_cell_E\t:<f4\n",
      "normalized_track_pt_cell_E\t:<f4\n"
     ]
    }
   ],
   "source": [
    "### dataset content and types\n",
    "\n",
    "# n_samples and points_per_sample\n",
    "print(\"Data format and shape:\")\n",
    "print(f\"{type(events)}\\t{events.shape=}\\n\\n\")\n",
    "# Note: structured numpy array --> columns accessible by name\n",
    "\n",
    "# dataset columns\n",
    "print(f\"Column\\tdtype\")\n",
    "for colname, coltype in events.dtype.descr:\n",
    "    print(f\"{colname}\\t:{coltype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6dec974-e20d-424d-80f4-13f7c3dc1e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_events=314\n",
      "n_samples=325\n",
      "n_points_per_sample=800\n"
     ]
    }
   ],
   "source": [
    "# dataset statistics\n",
    "\n",
    "n_samples, n_points_per_sample = events.shape\n",
    "\n",
    "sample_event_ids = events['event_number'][:,0]\n",
    "event_ids, event_ids_count = np.unique(sample_event_ids, return_counts=True)\n",
    "n_events = len(event_ids)\n",
    "\n",
    "print(f\"{n_events=}\")\n",
    "print(f\"{n_samples=}\")\n",
    "print(f\"{n_points_per_sample=}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
