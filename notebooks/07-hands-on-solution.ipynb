{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d423bb-e164-4cd2-9f3e-ca3e265b3925",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "# MLOps\n",
    "\n",
    "Now that you've seen the individual components of an MLOps workflow with W&B, it's time to apply everything you've learned in a complete pipeline. You'll combine data versioning, experiment tracking, and hyperparameter optimization in a single comprehensive exercise.\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Building on what we covered in previous notebooks, implement a complete MLOps pipeline that:\n",
    "\n",
    "1. **Data Management**\n",
    "   - Filter the dataset to keep only single-track events\n",
    "   - Create versioned artifacts (`train_data:v1` and `val_data:v1`) with \"one-track\" alias, `run.log_artifact(, aliases=[..., \"one-track\"])`\n",
    "   - Document your data processing decisions\n",
    "\n",
    "2. **Training Pipeline**\n",
    "   - Use sample_weights argument of model.fit (ensure dataset is created appropriately, i.e. x, y, sample_weigths format)\n",
    "   - Use proper experiment tracking (metrics, gradients, model checkpoints)\n",
    "   - Save model versions\n",
    "\n",
    "3. **Optimization**\n",
    "   - Design and execute a sweep of your choice\n",
    "   - Must include at least 3 hyperparameters to optimize\n",
    "   - Analyze and document the results\n",
    "\n",
    "**Feel free to explore W&B documentation to use more advanced features!** \n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "# PointNet for particle flow\n",
    "\n",
    "## Problem\n",
    "\n",
    "This dataset contains a Monte Carlo simulation of $\\rho^{\\pm} \\rightarrow \\pi^{\\pm} + \\pi^0$ decays and the corresponding detector response. Specifically, the data report the measured response of **i) tracker** and **ii) calorimeter**, along with the true pyshical quantitites that generated those measurements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This means that we expect one track per event, with mainly two energy blobs (clusters of cells) in the calorimeter.\n",
    "</div>\n",
    "\n",
    "The final **goal** is to associate the cell signals observed in the calorimeter to the track that caused those energy deposits.\n",
    "\n",
    "## Method\n",
    "\n",
    "The idea is to leverage a **point cloud** data representation to combine tracker and calorimeter information so to associate cell hits to the corresponding track. We will use a [**PointNet**](https://openaccess.thecvf.com/content_cvpr_2017/papers/Qi_PointNet_Deep_Learning_CVPR_2017_paper.pdf) model that is capable of handling this type of data, framed as a **semantic segmentation** approach. More precisely, this means that:\n",
    "- we represent each hit in the detector as a point in the point cloud: x, y, z coordinates + additional features (\"3+\"-dimensional point)\n",
    "- the **learning task** will be binary classification at hit level: for each cell the model learns whether its energy comes mostly from the track (class 1) or not (class 0)\n",
    "\n",
    "## Data structure\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "This dataset is organized as follows:\n",
    " - for each event, we create a **sample** (i.e. point cloud)\n",
    " - each sample contains all hits in a cone around a track of the event, called **focal track**\n",
    "     - the cone includes all hits within some $\\Delta R$ distance of the track\n",
    "     - if an event has multiple tracks, then we have more samples per event\n",
    "     - since different samples have possibly different number of hits, **we pad all point clouds to ensure they have same size** (needed since the model requires inputs of same size)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3576b6bf-eb45-4742-8f24-1023d6e0cc15",
   "metadata": {},
   "source": [
    "## Settings & config\n",
    "\n",
    "This section collects all configuration variables and training/model hyperparameters. \n",
    "\n",
    "The idea is to put it at the top so that it is easy to find and edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c725801-a326-486d-97ca-5a165b2bbb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 16:27:40.286415: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 16:27:42.224175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# path settings\n",
    "REPO_BASEPATH = Path().cwd().parent\n",
    "DATA_PATH = REPO_BASEPATH / \"pnet_data/raw/rho_small.npz\"\n",
    "CODE_PATH = REPO_BASEPATH / \"src\"\n",
    "sys.path.append(str(CODE_PATH))\n",
    "MODEL_CHECKPOINTS_PATH = REPO_BASEPATH / \"results\" / \"models\" / \"pointnet_baseline.weights.h5\"\n",
    "\n",
    "import wandb\n",
    "from data_viz import *\n",
    "from model_utils import *\n",
    "\n",
    "LABELS = [\"unfocus hit\", \"focus hit\"]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "SEED = 18\n",
    "set_global_seeds(SEED)\n",
    "\n",
    "# data settings\n",
    "N_TRAIN, N_VAL, N_TEST = 210, 65, 50 # roughly 0.65, 0.2, 0.15\n",
    "\n",
    "# model settings\n",
    "N_FEATURES = 3\n",
    "INIT_SIZE = 8\n",
    "END_SIZE = 16\n",
    "\n",
    "# training settings\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "INIT_LR = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54eb0a-6709-42c1-8f92-7b1b84e7d9de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Model training\n",
    "\n",
    "We proceed with model training:\n",
    "\n",
    "1. split the data\n",
    "1. build our PointNet model using Tensorflow/Keras\n",
    "1. create a dataloader to feed batches into our model\n",
    "1. train\n",
    "1. check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776985d-5603-4049-b25b-92462ac5c187",
   "metadata": {},
   "source": [
    "### PointNet model \n",
    "\n",
    "We use a PointNet model for semantic segmentation. Here is an illustration of its structure:\n",
    "\n",
    "![PointNet architecture](../pnet_data/images/pointnet-architecture.jpg)\n",
    "\n",
    "We have two heads:\n",
    " - classification head (used for point cloud classification)\n",
    " - segmentation head (used for semantic segmentation)\n",
    "\n",
    "We are going to use the **segmentation head** for our problem. The architecture settings we can experiment with are:\n",
    " - `n_features` (the number of input features): original version has only size 3 as it only takes x,y,z coordinates\n",
    " - `init_size` (number of filters of first convolutional layer): original version has 64\n",
    " - `end_size` (number of filters in segmentation head): original version has 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e89ffc-4518-4c47-948c-b91c8bd16e52",
   "metadata": {},
   "source": [
    "### Utils for new settings\n",
    "\n",
    "You can try to implement new features alone, but you can check out some reference implementations in case you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb63250a-2125-478d-bf3e-a9ee39e0657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_track_filter(data, labels):\n",
    "    multiple_samples_event_ids = [826871, 827140, 827188, 827226, 827242, 828437]\n",
    "    \n",
    "    one_track_mask = ~np.any(np.isin(train_data['event_number'], multiple_samples_event_ids), axis=1)\n",
    "    filtered_data = data[one_track_mask]\n",
    "    filtered_labels = labels[one_track_mask]\n",
    "    return filtered_data, filtered_labels\n",
    "\n",
    "def augment(point_cloud_batch, label_cloud_batch):\n",
    "    noise = tf.random.uniform(\n",
    "        tf.shape(point_cloud_batch[:, :, :3]), -0.001, 0.001, dtype=tf.float64\n",
    "    )\n",
    "\n",
    "    noisy_xyz = point_cloud_batch[:, :, :3] + noise\n",
    "    point_cloud_batch = tf.concat([noisy_xyz, point_cloud_batch[:, :, 3:]], axis=-1)\n",
    "    \n",
    "    return point_cloud_batch, label_cloud_batch\n",
    "\n",
    "def get_coords_labels_weights(point_cloud_batch, label_cloud_batch):\n",
    "    category_mask = point_cloud_batch[:,:,3]\n",
    "    # assign weight=1 only to cell hits (category=1)\n",
    "    weights = tf.cast(tf.equal(category_mask, 1), tf.float32)\n",
    "    return point_cloud_batch[:,:,:3], label_cloud_batch, weights\n",
    "    \n",
    "def generate_dataset(point_clouds, label_clouds, is_training=True, bs=16, n_points=800, n_features=3, labels=[\"unfocus hit\", \"focus hit\"]):\n",
    "    # reformat to unstructured array and transform to list of size n_samples, each element of size n_ponints x n_features\n",
    "    point_clouds = structured_to_unstructured(point_clouds).astype(np.float64)\n",
    "    point_clouds = [_ for _ in point_clouds]\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((point_clouds, label_clouds))\n",
    "    dataset = dataset.shuffle(bs * 100) if is_training else dataset\n",
    "    load_data_with_args = partial(load_data, n_points=n_points, n_features=n_features, labels=labels)\n",
    "    dataset = dataset.map(load_data_with_args, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size=bs)\n",
    "    dataset = (\n",
    "        dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        if is_training\n",
    "        else dataset\n",
    "    )\n",
    "    dataset = dataset.map(get_coords_labels_weights, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba190c14-dc20-43ea-babd-48882167f0bc",
   "metadata": {},
   "source": [
    "### Filtering and versioning\n",
    "\n",
    "First step is to filter multi-track events out and update the data artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0b3fc-14ac-4626-939e-760bc9ef4eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_DATA_PATH = DATA_PATH.parent.parent\n",
    "NEW_SPLIT_PATH = SPLIT_DATA_PATH / \"one-track\"\n",
    "NEW_SPLIT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "def read_data(split, bin_cutoff=0.5, n_classes=2, split_data_path=SPLIT_DATA_PATH): \n",
    "    filepath=str(split_data_path / f\"{split}_data\" / DATA_PATH.name)\n",
    "    data = np.load(filepath)['feats']\n",
    "    target_class = [(energy_fraction > bin_cutoff).astype(np.float32) \n",
    "                    for energy_fraction in data['truth_cell_focal_fraction_energy']]\n",
    "    # target_class = (events[\"truth_cell_focal_fraction_energy\"] > 0.5).reshape(-1)\n",
    "    target_class = keras.utils.to_categorical(target_class, num_classes=n_classes)\n",
    "    return data, target_class\n",
    "\n",
    "def filter_and_version(split_data_path=NEW_SPLIT_PATH):\n",
    "\n",
    "    with wandb.init(project=\"mlops-ai_infn\", entity=\"lclissa\", \n",
    "                job_type=\"preproc\", notes=\"Filtering out multi-track events\") as run:\n",
    "        # filter data\n",
    "        run.use_artifact(\"train_data:latest\")\n",
    "        run.use_artifact(\"val_data:latest\")\n",
    "        \n",
    "        train_data, train_label_cloud = read_data(\"train\", split_data_path=SPLIT_DATA_PATH)\n",
    "        filtered_train_data, filtered_train_label_cloud = one_track_filter(train_data, train_label_cloud)\n",
    "        # save locally\n",
    "        filepath=str(split_data_path / \"train_data\" / DATA_PATH.name)\n",
    "        np.savez(filepath, feats=filtered_train_data)\n",
    "        # create new artifact tracking new version\n",
    "        filtered_train_artifact = wandb.Artifact(name=\"train_data\", type=\"dataset\", description=\"One track only\")\n",
    "        filtered_train_artifact.add_file(local_path = str(filepath))\n",
    "        run.log_artifact(filtered_train_artifact, aliases=[\"one-track\"])\n",
    "        \n",
    "        train_point_clouds = train_data[input_features]\n",
    "        total_training_examples = len(train_point_clouds)\n",
    "        \n",
    "        val_data, val_label_cloud = read_data(\"val\", split_data_path=SPLIT_DATA_PATH)\n",
    "        filtered_val_data, filtered_val_label_cloud = one_track_filter(val_data, val_label_cloud)\n",
    "        # save locally\n",
    "        filepath=str(split_data_path / \"val_data\" / DATA_PATH.name)\n",
    "        np.savez(filepath, feats=filtered_val_data)\n",
    "        # create new artifact tracking new version\n",
    "        filtered_val_artifact = wandb.Artifact(name=\"val_data\", type=\"dataset\", description=\"One track only\")\n",
    "        filtered_val_artifact.add_file(local_path = str(filepath))\n",
    "        run.log_artifact(filtered_val_artifact, aliases=[\"one-track\"])\n",
    "\n",
    "print(f\"Creating new data version at: {NEW_SPLIT_PATH}\")\n",
    "filter_and_version(NEW_SPLIT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534d22c-3887-4ce2-98da-c69909a95bfb",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now you need to implement the training function to be passed to the sweep. A basic structure should be the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "562a4df7-ddea-4312-bcfe-88add3009474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "    \n",
    "def train_wrapper():\n",
    "    run = wandb.init(project=\"mlops-ai_infn\", entity=\"lclissa\", # name=\"first-sweep\", # not needed as the sweep takes care of this\n",
    "                job_type=\"sweep\", notes=\"Playing with sweeps ...\")\n",
    "    cfg = run.config\n",
    "    _ = run.use_artifact(\"train_data:latest\")\n",
    "    _ = run.use_artifact(\"val_data:latest\")\n",
    "    \n",
    "    input_features = [\"normalized_x\", \"normalized_y\", \"normalized_z\", \"category\"]\n",
    "\n",
    "    train_data, train_label_cloud = read_data(\"train\", split_data_path=NEW_SPLIT_PATH)\n",
    "    \n",
    "    train_point_clouds = train_data[input_features]\n",
    "    total_training_examples = len(train_point_clouds)\n",
    "    \n",
    "    val_data, val_label_cloud = read_data(\"val\", split_data_path=NEW_SPLIT_PATH)\n",
    "    val_point_clouds = val_data[input_features]\n",
    "    \n",
    "    print(\"Num train point clouds:\", len(train_point_clouds))\n",
    "    print(\"Num train point cloud labels:\", len(train_label_cloud))\n",
    "    print(\"Num val point clouds:\", len(val_point_clouds))\n",
    "    print(\"Num val point cloud labels:\", len(val_label_cloud))\n",
    "    \n",
    "    n_points = train_point_clouds[0].shape[0]\n",
    "    n_features = len(train_point_clouds[0].dtype.names)\n",
    "    n_classes = len(LABELS)\n",
    "\n",
    "    \n",
    "    train_dataset = generate_dataset(train_point_clouds, train_label_cloud, \n",
    "                                 bs=cfg.batch_size, n_points=n_points, n_features=n_features, labels=LABELS)\n",
    "    val_dataset = generate_dataset(val_point_clouds, val_label_cloud, is_training=False, \n",
    "                                   bs=cfg.batch_size, n_points=n_points, n_features=n_features, labels=LABELS)\n",
    "    \n",
    "    steps_per_epoch = total_training_examples // cfg.batch_size\n",
    "    total_training_steps = steps_per_epoch * EPOCHS\n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=cfg.init_lr,\n",
    "        decay_steps=steps_per_epoch * 5,\n",
    "        decay_rate=0.5,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    segmentation_model = get_shape_segmentation_model(n_points, n_classes, n_features-1,\n",
    "                                                      INIT_SIZE, END_SIZE)\n",
    "    segmentation_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "        jit_compile=False\n",
    "    )\n",
    "\n",
    "    MODEL_CHECKPOINTS_PATH.parent.mkdir(exist_ok=True, parents=True)\n",
    "    history = segmentation_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[\n",
    "            WandbMetricsLogger(log_freq=5),\n",
    "            WandbModelCheckpoint(\n",
    "                       MODEL_CHECKPOINTS_PATH, #.parent / \"model-{epoch:02d}-{val_loss:.2f}.weights.h5\",\n",
    "                       monitor=\"val_loss\",\n",
    "                       save_best_only=True,\n",
    "                       save_weights_only=True,\n",
    "                   )\n",
    "        ],\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8a87203-a5b0-4c29-bbce-38a872237672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m To avoid this, please fix the sweep config schema violations below:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m   Violation 1. init_lr uses log_uniform, where min/max specify base-e exponents. Use log_uniform_values to specify limit values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: w01p3u72\n",
      "Sweep URL: https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: okhgi8e4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_lr: 1.001890095488338\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_size: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: ğŸš€ View run \u001b[33mupbeat-sweep-2\u001b[0m at: \u001b[34mhttps://wandb.ai/lclissa/mlops-ai_infn/runs/259zu2t7\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241127_160235-259zu2t7/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/private/mlops-handson/notebooks/wandb/run-20241127_160556-okhgi8e4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/okhgi8e4' target=\"_blank\">dashing-sweep-1</a></strong> to <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/okhgi8e4' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/okhgi8e4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train point clouds: 210\n",
      "Num train point cloud labels: 210\n",
      "Num val point clouds: 65\n",
      "Num val point cloud labels: 65\n",
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Epoch 1/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - accuracy: 0.5577 - loss: 164543168.0000 - val_accuracy: 0.1266 - val_loss: inf\n",
      "Epoch 2/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.2237 - loss: 280867328.0000 - val_accuracy: 0.1266 - val_loss: inf\n",
      "Epoch 3/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7316 - loss: 16498151.0000 - val_accuracy: 0.1266 - val_loss: inf\n",
      "Epoch 4/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.3094 - loss: 4951713.5000 - val_accuracy: 0.1266 - val_loss: 71749880627424969607863631618244608.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.7420 - loss: 1994790.1250 - val_accuracy: 0.8734 - val_loss: 23365576642744365334850701361152.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.3270 - loss: 1130955.8750 - val_accuracy: 0.8820 - val_loss: 140384659635087528011830919168.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8393 - loss: 720328.7500 - val_accuracy: 0.1266 - val_loss: 3183905370791121994746167296.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.5702 - loss: 441233.0000 - val_accuracy: 0.1266 - val_loss: 75149987767700531523354624.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1273 - loss: 392344.9375 - val_accuracy: 0.1266 - val_loss: 2796499596149385915793408.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8721 - loss: 325194.4062 - val_accuracy: 0.1266 - val_loss: 237278469421891409936384.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1192 - loss: 322070.6562 - val_accuracy: 0.1266 - val_loss: 26701497665308176941056.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6791 - loss: 253524.7031 - val_accuracy: 0.1266 - val_loss: 4370018640298036428800.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.1242 - loss: 1248531.7500 - val_accuracy: 0.1266 - val_loss: 1359607594843063189504.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.8628 - loss: 4255834.0000 - val_accuracy: 0.1266 - val_loss: 50065043711896584192.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.1331 - loss: 1387907.3750 - val_accuracy: 0.1266 - val_loss: 6068001264044605440.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.7088 - loss: 847430.4375 - val_accuracy: 0.1266 - val_loss: 1138542435136176128.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8743 - loss: 500884.7812 - val_accuracy: 0.1266 - val_loss: 213841027188916224.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - accuracy: 0.4130 - loss: 264184.0000 - val_accuracy: 0.1266 - val_loss: 57661864389115904.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.6624 - loss: 248365.4531 - val_accuracy: 0.1266 - val_loss: 8903696985358336.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.8739 - loss: 177535.6875 - val_accuracy: 0.1266 - val_loss: 1843235278290944.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>â–‡â–†â–‚â–‚â–‚â–ˆâ–…â–â–â–„â–â–ˆâ–‡â–â–†â–â–â–ˆâ–ˆâ–â–â–â–‡â–‡â–â–â–ˆâ–ˆâ–‚â–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‚â–â–‡â–ˆâ–ˆ</td></tr><tr><td>batch/batch_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>batch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>batch/loss</td><td>â–â–‚â–ˆâ–…â–„â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/accuracy</td><td>â–†â–ƒâ–…â–…â–…â–…â–†â–†â–â–ˆâ–â–†â–â–‡â–â–‡â–ˆâ–‚â–‡â–ˆ</td></tr><tr><td>epoch/epoch</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–â–â–â–ˆâ–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_loss</td><td>   â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.88001</td></tr><tr><td>batch/batch_step</td><td>295</td></tr><tr><td>batch/learning_rate</td><td>0.06262</td></tr><tr><td>batch/loss</td><td>192260.85938</td></tr><tr><td>epoch/accuracy</td><td>0.87831</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.06262</td></tr><tr><td>epoch/loss</td><td>196098.57812</td></tr><tr><td>epoch/val_accuracy</td><td>0.12656</td></tr><tr><td>epoch/val_loss</td><td>1843235278290944.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-sweep-1</strong> at: <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/okhgi8e4' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/okhgi8e4</a><br/> View project at: <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a><br/>Synced 5 W&B file(s), 0 media file(s), 36 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241127_160556-okhgi8e4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xkoq2h98 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_lr: 1.0027811617519489\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg entity when running a sweep.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/private/mlops-handson/notebooks/wandb/run-20241127_160703-xkoq2h98</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/xkoq2h98' target=\"_blank\">astral-sweep-2</a></strong> to <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/sweeps/w01p3u72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/xkoq2h98' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/xkoq2h98</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train point clouds: 210\n",
      "Num train point cloud labels: 210\n",
      "Num val point clouds: 65\n",
      "Num val point cloud labels: 65\n",
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "<class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
      "Epoch 1/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 99ms/step - accuracy: 0.5139 - loss: 130938392.0000 - val_accuracy: 0.1266 - val_loss: inf\n",
      "Epoch 2/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.3112 - loss: 890865728.0000 - val_accuracy: 0.1266 - val_loss: inf\n",
      "Epoch 3/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.4931 - loss: 32701370.0000 - val_accuracy: 0.8734 - val_loss: inf\n",
      "Epoch 4/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2740 - loss: 4065840.2500 - val_accuracy: 0.8734 - val_loss: 34550737246840217424350098227200.0000\n",
      "Epoch 5/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.3032 - loss: 1623328.6250 - val_accuracy: 0.8734 - val_loss: 822558986351711500623675392.0000\n",
      "Epoch 6/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.8098 - loss: 928025.6250 - val_accuracy: 0.8734 - val_loss: 11770574551371894747037696.0000\n",
      "Epoch 7/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - accuracy: 0.2843 - loss: 748262.5625 - val_accuracy: 0.8734 - val_loss: 271244779941106276106240.0000\n",
      "Epoch 8/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.3203 - loss: 689589.3125 - val_accuracy: 0.8734 - val_loss: 14498660082675485769728.0000\n",
      "Epoch 9/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.6969 - loss: 535340.3125 - val_accuracy: 0.8734 - val_loss: 1417272106684380086272.0000\n",
      "Epoch 10/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 60ms/step - accuracy: 0.4312 - loss: 382231.5000 - val_accuracy: 0.8734 - val_loss: 85110556278233497600.0000\n",
      "Epoch 11/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.8765 - loss: 408722.9688 - val_accuracy: 0.8734 - val_loss: 4046357271599382528.0000\n",
      "Epoch 12/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.2880 - loss: 293988.4688 - val_accuracy: 0.8734 - val_loss: 912760814748827648.0000\n",
      "Epoch 13/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.4274 - loss: 289240.0938 - val_accuracy: 0.8734 - val_loss: 57075687252557824.0000\n",
      "Epoch 14/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 66ms/step - accuracy: 0.2954 - loss: 327417.0312 - val_accuracy: 0.8734 - val_loss: 50547749279498240.0000\n",
      "Epoch 15/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8805 - loss: 497746.4375 - val_accuracy: 0.8734 - val_loss: 564636621850506428416.0000\n",
      "Epoch 16/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5710 - loss: 12938764.0000 - val_accuracy: 0.8734 - val_loss: 26839065639872626688.0000\n",
      "Epoch 17/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5001 - loss: 3595569.0000 - val_accuracy: 0.8734 - val_loss: 176743882824876032.0000\n",
      "Epoch 18/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7027 - loss: 1016785.6875 - val_accuracy: 0.8734 - val_loss: 4040971788484608.0000\n",
      "Epoch 19/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - accuracy: 0.4378 - loss: 603077.9375 - val_accuracy: 0.8734 - val_loss: 7006092525568.0000\n",
      "Epoch 20/20\n",
      "\u001b[1m14/14\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - accuracy: 0.7733 - loss: 604827.0000 - val_accuracy: 0.2764 - val_loss: 6395035136.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>â–ƒâ–…â–†â–â–â–‡â–„â–ƒâ–â–â–ˆâ–‚â–‚â–ˆâ–ˆâ–‚â–‚â–ˆâ–‚â–‚â–â–ƒâ–‡â–ˆâ–ˆâ–‚â–â–ƒâ–…â–ˆâ–‚â–ˆâ–ˆâ–ˆâ–†â–…â–â–ƒâ–ˆâ–†</td></tr><tr><td>batch/batch_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>batch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>batch/loss</td><td>â–â–‚â–‚â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/accuracy</td><td>â–…â–„â–‚â–„â–â–‡â–‚â–‚â–„â–†â–ˆâ–â–†â–â–ˆâ–ƒâ–†â–„â–†â–…</td></tr><tr><td>epoch/epoch</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–…â–†â–†â–‡â–‡â–‡â–ˆâ–ˆ</td></tr><tr><td>epoch/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–„â–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–</td></tr><tr><td>epoch/loss</td><td>â–…â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/val_accuracy</td><td>â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‚</td></tr><tr><td>epoch/val_loss</td><td>   â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.66784</td></tr><tr><td>batch/batch_step</td><td>295</td></tr><tr><td>batch/learning_rate</td><td>0.06267</td></tr><tr><td>batch/loss</td><td>578623.8125</td></tr><tr><td>epoch/accuracy</td><td>0.57419</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.06267</td></tr><tr><td>epoch/loss</td><td>561374.5625</td></tr><tr><td>epoch/val_accuracy</td><td>0.2764</td></tr><tr><td>epoch/val_loss</td><td>6395035136.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">astral-sweep-2</strong> at: <a href='https://wandb.ai/lclissa/mlops-ai_infn/runs/xkoq2h98' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn/runs/xkoq2h98</a><br/> View project at: <a href='https://wandb.ai/lclissa/mlops-ai_infn' target=\"_blank\">https://wandb.ai/lclissa/mlops-ai_infn</a><br/>Synced 5 W&B file(s), 0 media file(s), 30 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241127_160703-xkoq2h98/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2: Define the search space\n",
    "sweep_configuration = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"epoch/val_loss\"},\n",
    "    \"parameters\": {\n",
    "        \"init_lr\": {'min': 1e-4, 'max': 1e-2,\n",
    "                    'distribution': 'log_uniform' },\n",
    "        \"batch_size\": {\"values\": [16, 32]},\n",
    "        \"init_size\": {\"value\": [64]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"mlops-ai_infn\", entity=\"lclissa\")\n",
    "\n",
    "wandb.agent(sweep_id, function=train_wrapper, count=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
